import json
import boto3
import pandas as pd
import pymysql
import io
import os
from datetime import datetime

def fmt_inr(x):
    return "â‚¹{:,.2f}".format(x)

s3 = boto3.client('s3')
sns_client = boto3.client('sns')

def lambda_handler(event, context):
    # 1) GET S3 object that triggered
    record = event['Records'][0]
    bucket = record['s3']['bucket']['name']
    key = record['s3']['object']['key']

    # get file from s3
    obj = s3.get_object(Bucket=bucket, Key=key)
    df_raw = pd.read_csv(io.BytesIO(obj['Body'].read()))

    # 2) CLEAN DATA
    df_raw['total_value'] = df_raw['quantity'] * df_raw['price']
    df_clean = df_raw.copy()

    # *** FIX NaN ***
    df_clean = df_clean.fillna(0)
    df_clean = df_clean.where(pd.notnull(df_clean), None)

    # write cleaned to processed folder
    cleaned_key = "processed/cleaned_orders_detailed.csv"
    csv_buffer = io.StringIO()
    df_clean.to_csv(csv_buffer, index=False)
    s3.put_object(
        Bucket=bucket,
        Key=cleaned_key,
        Body=csv_buffer.getvalue()
    )

    # 3) INSERT INTO RDS
    conn = pymysql.connect(
        host=os.environ['DB_HOST'],
        user=os.environ['DB_USER'],
        passwd=os.environ['DB_PASSWORD'],
        db=os.environ['DB_NAME'],
        connect_timeout=10
    )
    cur = conn.cursor()

    # create table if not exists
    create_sql = """
    CREATE TABLE IF NOT EXISTS orders_cleaned(
        order_id int,
        customer_name varchar(100),
        city varchar(50),
        product varchar(100),
        quantity int,
        price float,
        channel varchar(50),
        payment_mode varchar(50),
        discount_code varchar(20),
        order_date date,
        total_value float
    );
    """
    cur.execute(create_sql)
    conn.commit()

    insert_sql = """INSERT INTO orders_cleaned VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""

    rows = [tuple(x) for x in df_clean.to_numpy()]
    cur.executemany(insert_sql, rows)
    conn.commit()

    # 4) ADVANCED ANALYTICS FOR EXECUTIVE EMAIL

    total_revenue = df_clean['total_value'].sum()
    city_sales = df_clean.groupby('city')['total_value'].sum().sort_values(ascending=False)
    top3 = city_sales.head(3)
    category_sales = df_clean.groupby('product')['total_value'].sum().sort_values(ascending=False)
    top_category = category_sales.head(1).index[0]
    avg_order_value = df_clean['total_value'].mean()

    fmt_total = fmt_inr(total_revenue)
    fmt_avg = fmt_inr(avg_order_value)

    today = datetime.now().strftime("%Y%m%d")

    sns_message = f"""
DAILY BUSINESS PERFORMANCE SUMMARY


Region: India
Date: {today}

Total Revenue Today: {fmt_total}

Top 3 Cities by Revenue:
{top3.to_string()}

Top Performing Product Category:
{top_category}

Average Order Value (AOV):
{fmt_avg}

Action Recommendation:
Prioritize stock allocation towards the top cities and the {top_category} category.

This is a system generated executive summary.
"""

    sns_client.publish(
        TopicArn=os.environ['SNS_TOPIC_ARN'],
        Message=sns_message.strip()
    )

    # 5) WRITE SUMMARY JSON TO S3
    summary_obj = {
        "date": today,
        "total_revenue": total_revenue,
        "top_3_cities": top3.to_dict(),
        "top_category": top_category,
        "avg_order_value": avg_order_value
    }

    summary_key = f"analytics/summary_{today}.json"
    s3.put_object(
        Bucket=bucket,
        Key=summary_key,
        Body=json.dumps(summary_obj, indent=2)
    )

    return {
        "statusCode": 200,
        "body": "Pipeline success"
    }
