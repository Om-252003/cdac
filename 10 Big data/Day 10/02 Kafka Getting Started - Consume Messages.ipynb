{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "96046942-9387-42d3-8441-e5fbea1408e9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ZqMlwq23wdpR"
      },
      "source": [
        "\n",
        "### Overview\n",
        "This guide outlines the procedure for consuming and validating data from a Kafka topic. It is designed to ensure that data flowing into Kafka is correctly formatted and intact, which is crucial for downstream processing and analytics. This process involves connecting to Kafka, consuming data, performing validations, and handling potential errors effectively.\n",
        "\n",
        "### Prerequisites\n",
        "Ensure the following prerequisites are met before running this notebook:\n",
        "- **Kafka Setup:** Confirm that Kafka is set up and data is being produced into a Kafka topic. This should have been completed in the notebook **01 Kafka Getting Started - Produce Messages**.\n",
        "\n",
        "### Validate Data using Consumer\n",
        "Follow these steps to consume and validate the data in your Kafka topic:\n",
        "\n",
        "1. **Connect to Confluent Kafka:**\n",
        "   - Establish a connection to your Confluent Kafka environment. Ensure that you have the necessary configurations and credentials to connect successfully.\n",
        "\n",
        "2. **Consume Data:**\n",
        "   - Utilize the relevant APIs to start consuming messages from the Kafka topic.\n",
        "   - You may want to write a script or use existing tools to consume a few messages as a test.\n",
        "\n",
        "3. **Data Validation:**\n",
        "   - Verify the integrity and accuracy of the data consumed from the Kafka topic.\n",
        "   - Check for specific attributes or values in the messages to ensure they meet the expected criteria.\n",
        "\n",
        "4. **Logging and Output:**\n",
        "   - Optionally, log the results of your validation or output them to a file or screen to review the consumed messages.\n",
        "\n",
        "### Additional Notes\n",
        "- Remember to handle any errors or exceptions that may occur during the connection or consumption processes.\n",
        "- Ensure that your consumer script is configured to connect to the correct Kafka cluster and topic.\n",
        "\n",
        "### Conclusion\n",
        "Implementing thorough validation checks on data consumed from Kafka not only ensures data quality but also mitigates the risks associated with data corruption or misconfiguration. This step-by-step guide provides a systematic approach to verify and validate data integrity in real-time streaming applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "da5adb3d-a09f-4722-9c79-a2e7b66a99c5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQYZgECowdpU",
        "outputId": "8d9ecac0-0d52-4588-8763-375ce9d5b58e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: confluent_kafka in /usr/local/lib/python3.12/dist-packages (2.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install confluent_kafka\n",
        "from confluent_kafka import Consumer, KafkaError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2677fbbb-ecb1-4c3c-9958-561beb4a4415",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ww_-26UvrU5b"
      },
      "outputs": [],
      "source": [
        "# Kafka and Confluent Cloud Configuration\n",
        "kafka_bootstrap_servers = \"pkc-n98pk.us-west-2.aws.confluent.cloud:9092\"\n",
        "kafka_topic = \"users\"\n",
        "kafka_api_key = \"4ZXD5BX7OFKQFYQH\"\n",
        "kafka_api_secret = \"cfltFhzEUePcMAJVO+YPKEX5U2Xh9/PGkmycVSHm4Dwys6kUeOycQUVBw3BDmBwA\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c1e1ec8f-089e-4c5c-8f30-2e6871b4e3c9",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "YYp3Zv_WwdpV"
      },
      "outputs": [],
      "source": [
        "# Consumer configuration\n",
        "conf = {\n",
        "    'bootstrap.servers': kafka_bootstrap_servers,\n",
        "    'sasl.mechanisms': 'PLAIN',\n",
        "    'security.protocol': 'SASL_SSL',\n",
        "    'sasl.username': kafka_api_key,\n",
        "    'sasl.password': kafka_api_secret,\n",
        "    'group.id': 'user-consumer-group',\n",
        "    'auto.offset.reset': 'latest'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f385e64-4d66-4475-8d45-b7beef8e5cca",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "CfpmDXVPwdpW"
      },
      "outputs": [],
      "source": [
        "# Create Consumer instance\n",
        "consumer = Consumer(conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b74030b0-7cbf-4857-b61f-0a0ed8b03029",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "4Vp1r9IJwdpX"
      },
      "outputs": [],
      "source": [
        "# Subscribe to topic\n",
        "consumer.subscribe([kafka_topic])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d3995eb7-708e-41c3-95ab-cccb1f1f3cce",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEzcoq_0wdpX",
        "outputId": "fceae7ad-6105-4ab2-bdc6-0e9d897615ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received message: hiii\n",
            "Received message: hiii\n",
            "Received message: Demo first\n",
            "Received message: Demo second\n",
            "Received message: Demo third\n",
            "Received message: Demo fourth\n"
          ]
        }
      ],
      "source": [
        "# Read messages from Kafka\n",
        "try:\n",
        "    while True:\n",
        "        msg = consumer.poll(1.0)  # Timeout of 1 second\n",
        "\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                # End of partition event\n",
        "                print(f\"{msg.topic()} [{msg.partition()}] reached end at offset {msg.offset()}\")\n",
        "            elif msg.error():\n",
        "                raise KafkaException(msg.error())\n",
        "        else:\n",
        "            # Proper message\n",
        "            print(f\"Received message: {msg.value().decode('utf-8')}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "finally:\n",
        "    # Close down consumer to commit final offsets.\n",
        "    consumer.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "50c25b37-fb0a-4f79-aca1-c510f6804264",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "Sz9FAezKwdpX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 809349469570865,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "02 Kafka Getting Started - Consume Messages",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}