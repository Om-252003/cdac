# -*- coding: utf-8 -*-
"""d3-1-confusion-matrix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UCYLQVprcS5kZ4p_dwITpCYL_LL2f4YA
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

df = pd.read_csv('/content/Social_Network_Ads.csv')
df.shape

df.value_counts()

df.describe()

# to check correlation use ANOVA test because purchased is a categorical numerical column.

from scipy.stats import f_oneway

# H0 - No correlation
# H1 - Is correlation

grpAge = df.groupby('Purchased')['Age']

p = f_oneway(*grpAge.apply(list))[1]

round(p,3)

grpSalary = df.groupby('Purchased')['Salary']
p2 = f_oneway(*grpSalary.apply(list))[1]
round(p2,3)

# Reject Null Hypothesis means there is correlation between Age to Purchased and Salary to Purchased.

# The salaries are on the scale of 1000s so we can do :
#  1) standardized scaling  ( z - score scaling)
#  2) MinMax scaling

# because model can give more importance to salary than age because of it's magnitude.

# Here in this example we will do Z - score scaling

# z = ( x - mean ) /  std

# output is in range of -3 to 3 because data should lie between mean + 3 std  and mean - 3 std

# anything smaller / greater than that is outlier

# In Minmax scaling it is between 0 and 1

# The transformation is given by:

#    scaling the original data ->               X_std = (X - X.min) / (X.max - X.min)

#    rescaling the data to get original form -> X_scaled = X_std * (max - min) + min

# X.min means min value of X
# likewise,

from sklearn.preprocessing import StandardScaler, MinMaxScaler

x = df.drop('Purchased',axis=1).values
y = df['Purchased'].values

print(x.shape,y.shape)

sc = StandardScaler()
xsc = sc.fit_transform(x)
# fit calculates mean and std of both columns of the df

xsc.shape

xsc[0:5]

sc.inverse_transform(xsc[0:5])

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

xtrain,xtest,ytrain,ytest = train_test_split(xsc,y,test_size=0.2,random_state=0)
print(xtrain.shape,xtest.shape,ytrain.shape,ytest.shape)

model = LogisticRegression()
model.fit(xtrain,ytrain)
model.score(xtest,ytest)  # It will internally call predict and accuracy_score

# slash is used not to end line
from sklearn.metrics import confusion_matrix, accuracy_score, \
classification_report, ConfusionMatrixDisplay

yhat = model.predict(xtest)
accuracy_score(ytest,yhat)

cm = confusion_matrix(ytest,yhat) ; cm

# correct predictions of class 0 are 19, 1 are 27
# we tested 57 rows
# where there is 0 and predicted is also 0 the count is 19
# where there is 1 and predicted is also 1 the count is 27
# where these is 0 but predicted is 1 the count is 5
# where these is 1 but predicted is 0 the count is 6

print(classification_report(ytest,yhat))

# for 0 : precision -> 19 / 25 = 0.76
#         recall    -> 19 / 24 = 0.79

# for 1 : precision -> 27 / 32 = 0.84
#         recall    -> 27 / 33 = 0.82

ConfusionMatrixDisplay(cm).plot()

yhat  # by default it takes 0.5 as threshold

# here we will get prediction as a probability
yprob = model.predict_proba(xtest)  ; yprob

yhat

yhat2 = (yprob[:,1]>=0.5)  ; yhat2

yhat2 = (yprob[:,1]>=0.5).astype(int)  ; yhat2

(yhat == yhat2).sum()

print(round(accuracy_score(ytest,yhat2),2) * 100, "% accuracy")

print(classification_report(ytest,yhat2))
cm = confusion_matrix(ytest,yhat2) ;
print(cm)
ConfusionMatrixDisplay(cm).plot()

x = sc.transform([[45,100000]])
x

model.predict(x)

import seaborn as sns
sns.pairplot(df, hue='Purchased')

model.predict(sc.transform([[25,75000]]))

model.predict(sc.transform([[40,78423]]))

model.predict(sc.transform([[40,78422]]))

# Plot decision boundary

age = np.arange(np.min(xtest[:,0])-0.2, np.max(xtest[:,0])+0.2, 0.05)
salary = np.arange(np.min(xtest[:,1])-0.2, np.max(xtest[:,1])+0.2, 0.05)

# here -0.2 and +0.2 is just for padding so our plot's curves do not touch the axes.

AGE,SAL = np.meshgrid(age,salary)
print(AGE.shape, SAL.shape)

# In X rows values' are same
# In Y column values' are same

# Now we've got a mesh square with very fine grids
# And we need to flatten it,
# means every all rows will become 1 single row,

# ex.
# 1 2 3
# 4 5 6
# 7 8 9

# flatten -> [1 2 3 4 5 6 7 8 9]

feature1 = AGE.ravel()
feature2 = SAL.ravel()

test_vals = np.c_[feature1,feature2]
test_vals.shape

# concatenate as per model's need and reshape to 2D shape so that contour can be plot
# get predicted answers
pred_ans = model.predict(test_vals).reshape(AGE.shape)
pred_ans.shape

plt.contour(AGE,SAL,pred_ans)

plt.contourf(AGE,SAL,pred_ans)

plt.scatter(xtest[:,0],xtest[:,1])  # These are our 57 data points

plt.contourf(AGE,SAL,pred_ans)

plt.scatter(xtest[ytest==0,0],xtest[ytest==0,1], c='y',label='Not Purchased')
plt.scatter(xtest[ytest==1,0],xtest[ytest==1,1], c='g',label='Purchased')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)

fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection='3d')

ax.set_xlabel('Age')
ax.set_ylabel('Salary')
ax.set_zlabel('Purchased')

ax.plot_surface(AGE,SAL,pred_ans, cmap='viridis', alpha=0.8)  # here 'alpha=0.8' means transparency

